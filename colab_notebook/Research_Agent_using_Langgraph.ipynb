{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openrouter_api_key = userdata.get('openrouter')"
      ],
      "metadata": {
        "id": "nfsJbc0Otrf8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iT3XDYkDPF1X",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q langgraph langchain langchain-openai\n",
        "!pip install langgraph langchain-openai langchain-anthropic langchain-community duckduckgo-search arxiv chromadb pydantic\n",
        "!pip install langgraph-checkpoint-sqlite\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvOBowg4dnCt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Standard Python ----------\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import hashlib\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "from dataclasses import dataclass, field\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Dict, Any, Optional, TypedDict, Annotated\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "# ---------- Pydantic ----------\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "\n",
        "# ---------- LangGraph ----------\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "\n",
        "# ---------- LLM Providers ----------\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# ---------- Tools / Search ----------\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "\n",
        "\n",
        "# ---------- Document Loading ----------\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "\n",
        "# ---------- Text Splitters (FIXED) ----------\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# ---------- Vector Stores ----------\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "\n",
        "# ---------- Chains ----------\n",
        "# from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "# ---------- Prompts & Messages ----------\n",
        "from langchain_classic.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "# ---------- Runnables ----------\n",
        "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
        "\n",
        "\n",
        "# ---------- Callbacks ----------\n",
        "# from langchain_core.callbacks import BaseCallbackHandler\n",
        "# from langchain_community.callbacks import FileCallbackHandler\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
      ],
      "metadata": {
        "id": "ALVad5tfl5m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d059db-454c-4e70-c696-980787a12ee7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchTopic(BaseModel):\n",
        "    title: str = Field(..., min_length=5, max_length=200)\n",
        "    domain: str = Field(..., description=\"e.g., 'AI', 'Physics', 'Biology'\")\n",
        "    complexity: str = Field(\"intermediate\", pattern=\"^(beginner|intermediate|advanced|expert)$\")\n",
        "    subtopics: List[str] = Field(default_factory=list)\n",
        "\n",
        "    @validator('title')\n",
        "    def title_must_be_meaningful(cls, v):\n",
        "        if len(v.split()) < 2:\n",
        "            raise ValueError(\"Title must be at least 2 words\")\n",
        "        return v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmephzlZMl-G",
        "outputId": "44fa53b5-1d53-4bcd-bb76-f55e0987a3b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3115356385.py:7: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  @validator('title')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchState(TypedDict):\n",
        "    messages: Annotated[List[Any], add_messages]\n",
        "    topic: ResearchTopic\n",
        "    research_phase: str\n",
        "    search_results: Dict[str, Any]\n",
        "    sources: List[Dict[str, str]]\n",
        "    analysis: Dict[str, Any]\n",
        "    literature_review: str\n",
        "    methodology: str\n",
        "    findings: List[Dict[str, Any]]\n",
        "    limitations: List[str]\n",
        "    recommendations: List[str]\n",
        "    citations: List[Dict[str, str]]\n",
        "    validation_errors: List[str]\n",
        "    metadata: Dict[str, Any]\n",
        "    agent_logs: List[Dict[str, Any]]\n"
      ],
      "metadata": {
        "id": "CkY--v88N7aJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentType(Enum):\n",
        "    \"\"\"Types of specialized agents\"\"\"\n",
        "    RESEARCH_COORDINATOR = \"research_coordinator\"\n",
        "    SEARCH_SPECIALIST = \"search_specialist\"\n",
        "    ANALYST = \"analyst\"\n",
        "    VALIDATOR = \"validator\"\n",
        "    SYNTHESIZER = \"synthesizer\"\n",
        "    CRITIC = \"critic\"\n",
        "    WRITER = \"writer\""
      ],
      "metadata": {
        "id": "LvUWjIcUO7Uo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AgentConfig:\n",
        "    \"\"\"Configuration for individual agents\"\"\"\n",
        "    agent_type: AgentType\n",
        "    llm_model: str = \"liquid/lfm-2.5-1.2b-instruct:free\"\n",
        "    temperature: float = 0.3\n",
        "    max_tokens: int = 2000\n",
        "    tools: List[Any] = field(default_factory=list)\n",
        "    system_prompt: str = \"\"\n",
        "    is_async: bool = False"
      ],
      "metadata": {
        "id": "tPvdoYOrPIHL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedResearch:\n",
        "  def __init__(self):\n",
        "    self.search_tool = DuckDuckGoSearchRun()\n",
        "    self.arxiv_wrapper = ArxivAPIWrapper()\n",
        "    self.cache = {}\n",
        "\n",
        "  def search_with_cache(self,query:str,max_results:int=5) -> Dict[str,any]:\n",
        "    cache_key = hashlib.md5(query.encode()).hexdigest() # Corrected from m5 to md5\n",
        "    if cache_key in self.cache:\n",
        "      return self.cache[cache_key]\n",
        "\n",
        "    try:\n",
        "      with ThreadPoolExecutor(max_workers=3) as executor: # Corrected typo: ThreadPoolEcecutor -> ThreadPoolExecutor\n",
        "        futures=[\n",
        "            executor.submit(self._web_search,query,max_results),\n",
        "            executor.submit(self._arxiv_search,query,max_results),\n",
        "            executor.submit(self._scholar_search,query,max_results),\n",
        "        ]\n",
        "        results=[]\n",
        "        for future in as_completed(futures):\n",
        "          try:\n",
        "            results.extend(future.result())\n",
        "          except Exception as e:\n",
        "            print(f\"Search error {e}\")\n",
        "      unique_results = self._deduplicate_results(results)\n",
        "      result_dict={\n",
        "          \"query\":query,\n",
        "          \"timestamp\":datetime.now().isoformat(),\n",
        "          \"results\":unique_results[:max_results]\n",
        "      }\n",
        "      self.cache[cache_key]=result_dict\n",
        "      return result_dict\n",
        "    except Exception as e: # Added missing except block\n",
        "      print(f\"An error occurred in search_with_cache: {e}\")\n",
        "      return {\n",
        "          \"query\": query,\n",
        "          \"timestamp\": datetime.now().isoformat(),\n",
        "          \"results\": [],\n",
        "          \"error\": str(e)\n",
        "      }\n",
        "\n",
        "  def _web_search(self,query:str,max_results:int) -> List[Dict]:\n",
        "    try:\n",
        "      results = self.search_tool.run(query)\n",
        "      return [{\n",
        "          \"source\":\"web\",\n",
        "          \"content\":results[:1000],\n",
        "          \"relevance_score\":0.8\n",
        "      }]\n",
        "    except:\n",
        "      return []\n",
        "\n",
        "  def _arxiv_search(self, query: str, max_results: int) -> List[Dict]:\n",
        "        \"\"\"Search arXiv for academic papers\"\"\"\n",
        "        try:\n",
        "            docs = self.arxiv_wrapper.load(query[:300])\n",
        "            return [{\n",
        "                \"source\": \"arxiv\",\n",
        "                \"title\": doc.metadata.get(\"Title\", \"\"),\n",
        "                \"authors\": doc.metadata.get(\"Authors\", []),\n",
        "                \"summary\": doc.page_content[:500],\n",
        "                \"published\": doc.metadata.get(\"Published\", \"\"),\n",
        "                \"relevance_score\": 0.9\n",
        "            } for doc in docs[:max_results]]\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "  def _scholar_search(self, query: str, max_results: int) -> List[Dict]: # Corrected indentation\n",
        "        \"\"\"Mock scholar search (replace with actual API)\"\"\"\n",
        "        return [{\n",
        "            \"source\": \"scholar\",\n",
        "            \"title\": f\"Research on {query}\",\n",
        "            \"abstract\": f\"This paper discusses {query} in detail...\",\n",
        "            \"citations\": 42,\n",
        "            \"relevance_score\": 0.85\n",
        "        }]\n",
        "\n",
        "  def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:\n",
        "    seen = set()\n",
        "    unique=[]\n",
        "    for result in results:\n",
        "      content_hash = hashlib.md5(str(result.get('title','') + result.get('content', '')).encode()).hexdigest()\n",
        "      if content_hash not in seen:\n",
        "        seen.add(content_hash)\n",
        "        unique.append(result) # Corrected: append result, not hash\n",
        "    return unique"
      ],
      "metadata": {
        "id": "pGJ8plWJPV0b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentAnalyzer:\n",
        "  def __init__(self):\n",
        "    self.Embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "  def analyze_content(self,content) -> Dict[str,any]:\n",
        "    chunks = self.text_splitter.split_text(content)\n",
        "    embeddings = self.Embeddings.embed_documents(chunks)\n",
        "    word_count = len(content.split())\n",
        "    sent_count =len(content.split('.'))\n",
        "    return {\n",
        "        \"chunks\": len(chunks),\n",
        "            \"word_count\": word_count,\n",
        "            \"sentence_count\": sent_count\n",
        "    }"
      ],
      "metadata": {
        "id": "Wfuo_7qzeJcm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6FotTrLlHBk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent:\n",
        "  def __init__(self,config = AgentConfig):\n",
        "    self.config = config\n",
        "    self.llm = self.initiate_llm()\n",
        "    self.tools = config.tools\n",
        "    self.logger = self._setup_logger()\n",
        "  def initiate_llm(self):\n",
        "    return ChatOpenAI(\n",
        "    openai_api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    model=\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
        ")\n",
        "  def _setup_logger(self):\n",
        "        \"\"\"Setup agent-specific logger\"\"\"\n",
        "        logger = logging.getLogger(self.config.agent_type.value)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        handler = logging.FileHandler(f\"logs/{self.config.agent_type.value}.log\")\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "  def log_activity(self, activity: str, metadata: Dict = None):\n",
        "        \"\"\"Log agent activity\"\"\"\n",
        "        log_entry = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"agent\": self.config.agent_type.value,\n",
        "            \"activity\": activity,\n",
        "            \"metadata\": metadata or {}\n",
        "        }\n",
        "        self.logger.info(json.dumps(log_entry))\n",
        "        return log_entry"
      ],
      "metadata": {
        "id": "yOUwMqJyj41_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchCoordinatorAgent(BaseAgent):\n",
        "    \"\"\"Orchestrates the research process\"\"\"\n",
        "    def __init__(self):\n",
        "        config = AgentConfig(\n",
        "            agent_type=AgentType.RESEARCH_COORDINATOR,\n",
        "            system_prompt=\"\"\"You are a Research Coordinator. Your responsibilities:\n",
        "            1. Analyze the research topic and break it into subtopics\n",
        "            2. Determine the appropriate research methodology\n",
        "            3. Coordinate between different specialist agents\n",
        "            4. Monitor research progress and quality\n",
        "            5. Synthesize final findings\n",
        "\n",
        "            You must ensure comprehensive coverage and academic rigor.\"\"\"\n",
        "        )\n",
        "        super().__init__(config)\n",
        "\n",
        "    async def coordinate(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Coordinate the research process\"\"\"\n",
        "        self.log_activity(\"coordination_started\", {\"topic\": state[\"topic\"].dict()})\n",
        "\n",
        "        # Analyze topic and create research plan\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.config.system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Research Topic: {state['topic'].title}\n",
        "            Domain: {state['topic'].domain}\n",
        "            Complexity: {state['topic'].complexity}\n",
        "\n",
        "            Create a comprehensive research plan including:\n",
        "            1. Key research questions\n",
        "            2. Required data sources\n",
        "            3. Methodology outline\n",
        "            4. Success criteria\n",
        "            5. Timeline estimate\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.llm\n",
        "        response = await chain.ainvoke({\n",
        "            \"messages\": state.get(\"messages\", [])\n",
        "        })\n",
        "\n",
        "        # Parse the response and update state\n",
        "        coordination_result = {\n",
        "            \"research_plan\": response.content,\n",
        "            \"next_phase\": \"literature_review\",\n",
        "            \"assigned_agents\": [\n",
        "                AgentType.SEARCH_SPECIALIST.value,\n",
        "                AgentType.ANALYST.value\n",
        "            ],\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.log_activity(\"coordination_completed\", coordination_result)\n",
        "\n",
        "        return {\"research_phase\": \"literature_review\", \"analysis\": coordination_result}\n",
        "\n",
        "class SearchSpecialistAgent(BaseAgent):\n",
        "    \"\"\"Specializes in finding and evaluating sources\"\"\"\n",
        "    def __init__(self):\n",
        "        config = AgentConfig(\n",
        "            agent_type=AgentType.SEARCH_SPECIALIST,\n",
        "            system_prompt=\"\"\"You are a Search Specialist. Your responsibilities:\n",
        "            1. Find relevant academic and web sources\n",
        "            2. Evaluate source credibility and relevance\n",
        "            3. Extract key information from sources\n",
        "            4. Maintain source citations\n",
        "            5. Identify knowledge gaps\n",
        "\n",
        "            Prioritize recent, authoritative sources.\"\"\"\n",
        "        )\n",
        "        super().__init__(config)\n",
        "        self.search_tool = AdvancedResearch()\n",
        "        self.content_analyzer = ContentAnalyzer()\n",
        "\n",
        "    async def search(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Perform comprehensive search\"\"\"\n",
        "        topic = state[\"topic\"]\n",
        "\n",
        "        # Generate search queries based on topic\n",
        "        queries = [\n",
        "            f\"{topic.title} recent developments\",\n",
        "            f\"{topic.domain} {topic.title} research papers\",\n",
        "            f\"{topic.title} methodology best practices\"\n",
        "        ]\n",
        "\n",
        "        # Execute parallel searches\n",
        "        search_tasks = [self._execute_search(query) for query in queries]\n",
        "        search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "        # Process and analyze results\n",
        "        processed_results = []\n",
        "        citations = []\n",
        "\n",
        "        for result_batch in search_results:\n",
        "            for result in result_batch.get(\"results\", []):\n",
        "                # Analyze content quality\n",
        "                if \"content\" in result or \"summary\" in result:\n",
        "                    content = result.get(\"content\", result.get(\"summary\", \"\"))\n",
        "                    analysis = self.content_analyzer.analyze_content(content)\n",
        "\n",
        "                    processed_result = {\n",
        "                        **result,\n",
        "                        \"analysis\": analysis,\n",
        "                        \"processed_at\": datetime.now().isoformat()\n",
        "                    }\n",
        "                    processed_results.append(processed_result)\n",
        "\n",
        "                    # Create citation\n",
        "                    citation = {\n",
        "                        \"id\": str(uuid.uuid4()),\n",
        "                        \"title\": result.get(\"title\", \"Untitled\"),\n",
        "                        \"authors\": result.get(\"authors\", [\"Unknown\"]),\n",
        "                        \"source\": result.get(\"source\", \"Unknown\"),\n",
        "                        \"url\": result.get(\"url\", \"\"),\n",
        "                        \"accessed_at\": datetime.now().isoformat()\n",
        "                    }\n",
        "                    citations.append(citation)\n",
        "\n",
        "        self.log_activity(\"search_completed\", {\n",
        "            \"queries\": queries,\n",
        "            \"results_found\": len(processed_results),\n",
        "            \"citations_generated\": len(citations)\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"search_results\": processed_results,\n",
        "            \"sources\": processed_results,\n",
        "            \"citations\": citations,\n",
        "            \"research_phase\": \"analysis\"\n",
        "        }\n",
        "\n",
        "    async def _execute_search(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute a single search query\"\"\"\n",
        "        return await asyncio.get_event_loop().run_in_executor(\n",
        "            None, self.search_tool.search_with_cache, query, 3\n",
        "        )\n",
        "\n",
        "class AnalystAgent(BaseAgent):\n",
        "    \"\"\"Analyzes and synthesizes information\"\"\"\n",
        "    def __init__(self):\n",
        "        config = AgentConfig(\n",
        "            agent_type=AgentType.ANALYST,\n",
        "            system_prompt=\"\"\"You are an Analysis Specialist. Your responsibilities:\n",
        "            1. Analyze collected data and sources\n",
        "            2. Identify patterns, trends, and relationships\n",
        "            3. Evaluate evidence quality and bias\n",
        "            4. Generate insights and hypotheses\n",
        "            5. Create structured analysis reports\n",
        "\n",
        "            Maintain objectivity and academic rigor.\"\"\"\n",
        "        )\n",
        "        super().__init__(config)\n",
        "\n",
        "    async def analyze(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze search results and generate insights\"\"\"\n",
        "        search_results = state.get(\"search_results\", [])\n",
        "        topic = state[\"topic\"]\n",
        "\n",
        "        # Prepare data for analysis\n",
        "        analysis_data = {\n",
        "            \"topic\": topic.dict(),\n",
        "            \"sources_count\": len(search_results),\n",
        "            \"sources_by_type\": {},\n",
        "            \"key_findings\": [],\n",
        "            \"methodologies_identified\": [],\n",
        "            \"knowledge_gaps\": []\n",
        "        }\n",
        "\n",
        "        # Categorize sources\n",
        "        for result in search_results:\n",
        "            source_type = result.get(\"source\", \"unknown\")\n",
        "            analysis_data[\"sources_by_type\"][source_type] = \\\n",
        "                analysis_data[\"sources_by_type\"].get(source_type, 0) + 1\n",
        "\n",
        "        # Generate analysis using LLM\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.config.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Analyze these research findings for topic: {topic.title}\n",
        "\n",
        "            Search Results Summary:\n",
        "            {json.dumps(search_results[:5], indent=2)}\n",
        "\n",
        "            Provide a comprehensive analysis covering:\n",
        "            1. Key trends and patterns\n",
        "            2. Conflicting viewpoints\n",
        "            3. Research methodologies used\n",
        "            4. Evidence strength assessment\n",
        "            5. Identified knowledge gaps\n",
        "            6. Preliminary conclusions\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.llm\n",
        "        response = await chain.ainvoke({})\n",
        "\n",
        "        # Parse and structure analysis\n",
        "        analysis_result = {\n",
        "            \"comprehensive_analysis\": response.content,\n",
        "            \"metadata\": analysis_data,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"confidence_score\": self._calculate_confidence(search_results)\n",
        "        }\n",
        "\n",
        "        self.log_activity(\"analysis_completed\", {\n",
        "            \"topic\": topic.title,\n",
        "            \"sources_analyzed\": len(search_results),\n",
        "            \"confidence\": analysis_result[\"confidence_score\"]\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"analysis\": analysis_result,\n",
        "            \"research_phase\": \"synthesis\",\n",
        "            \"findings\": self._extract_findings(response.content)\n",
        "        }\n",
        "\n",
        "    def _calculate_confidence(self, results: List[Dict]) -> float:\n",
        "        \"\"\"Calculate confidence score based on source quality\"\"\"\n",
        "        if not results:\n",
        "            return 0.0\n",
        "\n",
        "        scores = []\n",
        "        for result in results:\n",
        "            # Score based on source type and relevance\n",
        "            base_score = result.get(\"relevance_score\", 0.5)\n",
        "            source_bonus = {\n",
        "                \"arxiv\": 0.3,\n",
        "                \"scholar\": 0.3,\n",
        "                \"web\": 0.1\n",
        "            }.get(result.get(\"source\", \"\"), 0.0)\n",
        "            scores.append(min(1.0, base_score + source_bonus))\n",
        "\n",
        "        return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "    def _extract_findings(self, analysis: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract structured findings from analysis\"\"\"\n",
        "        # This is a simplified version - in production, use more sophisticated extraction\n",
        "        findings = []\n",
        "        lines = analysis.split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            if any(keyword in line.lower() for keyword in ['finding', 'conclusion', 'result', 'shows']):\n",
        "                findings.append({\n",
        "                    \"id\": str(uuid.uuid4()),\n",
        "                    \"content\": line.strip(),\n",
        "                    \"category\": self._categorize_finding(line),\n",
        "                    \"confidence\": 0.8\n",
        "                })\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def _categorize_finding(self, finding: str) -> str:\n",
        "        \"\"\"Categorize finding type\"\"\"\n",
        "        finding_lower = finding.lower()\n",
        "        if any(word in finding_lower for word in ['method', 'approach', 'technique']):\n",
        "            return \"methodology\"\n",
        "        elif any(word in finding_lower for word in ['result', 'finding', 'shows', 'indicates']):\n",
        "            return \"result\"\n",
        "        elif any(word in finding_lower for word in ['limitation', 'gap', 'challenge']):\n",
        "            return \"limitation\"\n",
        "        elif any(word in finding_lower for word in ['recommend', 'suggest', 'propose']):\n",
        "            return \"recommendation\"\n",
        "        return \"observation\""
      ],
      "metadata": {
        "id": "PbiBlBj4la-N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n"
      ],
      "metadata": {
        "id": "qXwuD68snUJq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aiosqlite # Import aiosqlite for async SQLite connections\n",
        "\n",
        "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        "from langgraph.checkpoint.base import BaseCheckpointSaver\n",
        "from langgraph.graph import StateGraph, END\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List, Optional\n",
        "import uuid\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Assuming ResearchState is defined elsewhere and necessary imports for agents\n",
        "\n",
        "class ResearchAssistantGraph:\n",
        "    \"\"\"Main research assistant graph with complex workflow\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Correctly instantiate AsyncSqliteSaver with an aiosqlite connection\n",
        "        self.memory = AsyncSqliteSaver(conn=aiosqlite.connect(\"research_checkpoints.db\"))\n",
        "        self.agents = self._initialize_agents()\n",
        "        self.graph = self._build_graph()\n",
        "        self.app = self._compile_graph()\n",
        "\n",
        "    def _initialize_agents(self) -> Dict[str, Any]:  # Changed from AgentType to str\n",
        "        \"\"\"Initialize all specialized agents\"\"\"\n",
        "        # Assuming these agent classes are defined elsewhere\n",
        "        return {\n",
        "            \"research_coordinator\": ResearchCoordinatorAgent(),\n",
        "            \"search_specialist\": SearchSpecialistAgent(),\n",
        "            \"analyst\": AnalystAgent(),\n",
        "            # Additional agents can be added here\n",
        "        }\n",
        "\n",
        "    def _build_graph(self) -> StateGraph:\n",
        "        \"\"\"Build the complex research workflow graph\"\"\"\n",
        "        workflow = StateGraph(ResearchState)\n",
        "\n",
        "        # Add nodes for each agent\n",
        "        workflow.add_node(\"research_coordinator\", self._create_agent_node(\"research_coordinator\"))\n",
        "        workflow.add_node(\"search_specialist\", self._create_agent_node(\"search_specialist\"))\n",
        "        workflow.add_node(\"analyst\", self._create_agent_node(\"analyst\"))\n",
        "        workflow.add_node(\"validator\", self._validator_node)\n",
        "        workflow.add_node(\"synthesizer\", self._synthesizer_node)\n",
        "        workflow.add_node(\"writer\", self._writer_node)\n",
        "\n",
        "        # Define the workflow edges with conditional routing\n",
        "        workflow.set_entry_point(\"research_coordinator\")\n",
        "\n",
        "        # Main research flow\n",
        "        workflow.add_edge(\"research_coordinator\", \"search_specialist\")\n",
        "        workflow.add_edge(\"search_specialist\", \"analyst\")\n",
        "\n",
        "        # Conditional routing based on analysis quality\n",
        "        workflow.add_conditional_edges(\n",
        "            \"analyst\",\n",
        "            self._route_based_on_quality,\n",
        "            {\n",
        "                \"continue\": \"validator\",\n",
        "                \"redo_search\": \"search_specialist\",\n",
        "                \"escalate\": \"research_coordinator\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Conditional routing based on validation results from validator\n",
        "        workflow.add_conditional_edges(\n",
        "            \"validator\",\n",
        "            self._route_from_validator,\n",
        "            {\n",
        "                \"analyst\": \"analyst\",       # If needs revision\n",
        "                \"synthesizer\": \"synthesizer\" # If validation passed\n",
        "            }\n",
        "        )\n",
        "\n",
        "        workflow.add_edge(\"synthesizer\", \"writer\")\n",
        "        workflow.add_edge(\"writer\", END)\n",
        "\n",
        "        return workflow\n",
        "\n",
        "    def _create_agent_node(self, agent_type: str):\n",
        "        \"\"\"Create a node for a specific agent type\"\"\"\n",
        "        agent = self.agents[agent_type]\n",
        "\n",
        "        async def agent_node(state: ResearchState):\n",
        "            if agent_type == \"research_coordinator\":\n",
        "                return await agent.coordinate(state)\n",
        "            elif agent_type == \"search_specialist\":\n",
        "                return await agent.search(state)\n",
        "            elif agent_type == \"analyst\":\n",
        "                return await agent.analyze(state)\n",
        "            else:\n",
        "                return {}\n",
        "\n",
        "        return agent_node\n",
        "\n",
        "    async def _validator_node(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Validate research quality and completeness\"\"\"\n",
        "        analysis = state.get(\"analysis\", {})\n",
        "        findings = state.get(\"findings\", [])\n",
        "\n",
        "        validation_results = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"checks\": [],\n",
        "            \"passed\": True,\n",
        "            \"issues\": []\n",
        "        }\n",
        "\n",
        "        # Check 1: Sufficient sources\n",
        "        sources_count = len(state.get(\"sources\", []))\n",
        "        if sources_count < 3:\n",
        "            validation_results[\"checks\"].append({\n",
        "                \"check\": \"minimum_sources\",\n",
        "                \"passed\": False,\n",
        "                \"message\": f\"Only {sources_count} sources found (minimum: 3)\"\n",
        "            })\n",
        "            validation_results[\"passed\"] = False\n",
        "            validation_results[\"issues\"].append(\"insufficient_sources\")\n",
        "\n",
        "        # Check 2: Analysis depth\n",
        "        analysis_text = analysis.get(\"comprehensive_analysis\", \"\")\n",
        "        if len(analysis_text.split()) < 200:\n",
        "            validation_results[\"checks\"].append({\n",
        "                \"check\": \"analysis_depth\",\n",
        "                \"passed\": False,\n",
        "                \"message\": f\"Analysis too brief: {len(analysis_text.split())} words\"\n",
        "            })\n",
        "            validation_results[\"passed\"] = False\n",
        "            validation_results[\"issues\"].append(\"shallow_analysis\")\n",
        "\n",
        "        # Check 3: Findings clarity\n",
        "        if len(findings) < 2:\n",
        "            validation_results[\"checks\"].append({\n",
        "                \"check\": \"findings_count\",\n",
        "                \"passed\": False,\n",
        "                \"message\": f\"Only {len(findings)} findings identified\"\n",
        "            })\n",
        "            validation_results[\"passed\"] = False\n",
        "            validation_results[\"issues\"].append(\"insufficient_findings\")\n",
        "\n",
        "        # Add passed checks\n",
        "        validation_results[\"checks\"].extend([\n",
        "            {\n",
        "                \"check\": \"topic_relevance\",\n",
        "                \"passed\": True,\n",
        "                \"message\": \"All findings are relevant to research topic\"\n",
        "            },\n",
        "            {\n",
        "                \"check\": \"citation_presence\",\n",
        "                \"passed\": True,\n",
        "                \"message\": f\"Found {len(state.get('citations', []))} citations\"\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            \"validation_errors\": validation_results.get(\"issues\", []),\n",
        "            \"validation_results\": validation_results,\n",
        "            \"research_phase\": \"synthesis\" if validation_results[\"passed\"] else \"needs_revision\"\n",
        "        }\n",
        "\n",
        "    def _route_based_on_quality(self, state: ResearchState) -> str:\n",
        "        \"\"\"Route based on analysis quality\"\"\"\n",
        "        analysis = state.get(\"analysis\", {})\n",
        "        confidence = analysis.get(\"confidence_score\", 0.0)\n",
        "\n",
        "        if confidence < 0.4:\n",
        "            return \"redo_search\"\n",
        "        elif confidence < 0.7:\n",
        "            return \"escalate\"\n",
        "        else:\n",
        "            return \"continue\"\n",
        "\n",
        "    def _route_from_validator(self, state: ResearchState) -> str:\n",
        "        \"\"\"Route based on validation results\"\"\"\n",
        "        if state.get(\"research_phase\") == \"needs_revision\":\n",
        "            return \"analyst\"\n",
        "        return \"synthesizer\"\n",
        "\n",
        "    async def _synthesizer_node(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Synthesize final research report\"\"\"\n",
        "        synthesis = {\n",
        "            \"executive_summary\": self._generate_executive_summary(state),\n",
        "            \"key_findings\": state.get(\"findings\", []),\n",
        "            \"methodology\": self._extract_methodology(state),\n",
        "            \"limitations\": self._identify_limitations(state),\n",
        "            \"recommendations\": self._generate_recommendations(state),\n",
        "            \"synthesized_at\": datetime.now().isoformat(),\n",
        "            \"total_sources\": len(state.get(\"sources\", [])),\n",
        "            \"validation_status\": state.get(\"validation_results\", {}).get(\"passed\", False)\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"literature_review\": self._create_literature_review(state),\n",
        "            \"methodology\": synthesis[\"methodology\"],\n",
        "            \"limitations\": synthesis[\"limitations\"],\n",
        "            \"recommendations\": synthesis[\"recommendations\"],\n",
        "            \"research_phase\": \"writing\"\n",
        "        }\n",
        "\n",
        "    async def _writer_node(self, state: ResearchState) -> Dict[str, Any]:\n",
        "        \"\"\"Generate final research paper\"\"\"\n",
        "        paper_sections = [\n",
        "            \"Title and Abstract\",\n",
        "            \"Introduction\",\n",
        "            \"Literature Review\",\n",
        "            \"Methodology\",\n",
        "            \"Findings and Analysis\",\n",
        "            \"Discussion\",\n",
        "            \"Limitations\",\n",
        "            \"Conclusion and Recommendations\",\n",
        "            \"References\"\n",
        "        ]\n",
        "\n",
        "        final_paper = {\n",
        "            \"title\": f\"Research Report: {state['topic'].title}\",\n",
        "            \"sections\": paper_sections,\n",
        "            \"content\": self._compile_final_paper(state),\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"word_count\": sum(len(section.split()) for section in paper_sections),\n",
        "            \"citations_count\": len(state.get(\"citations\", [])),\n",
        "            \"metadata\": state.get(\"metadata\", {})\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"final_paper\": final_paper,\n",
        "            \"research_phase\": \"completed\",\n",
        "            \"completion_time\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def _compile_graph(self):\n",
        "        \"\"\"Compile the graph with memory and parallel processing\"\"\"\n",
        "        # FIXED: Correct way to pass the checkpointer\n",
        "        return self.graph.compile(\n",
        "            checkpointer=self.memory,\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "    # Helper methods for synthesis and writing (keep these as is)\n",
        "    def _generate_executive_summary(self, state: ResearchState) -> str:\n",
        "        \"\"\"Generate executive summary\"\"\"\n",
        "        topic = state[\"topic\"]\n",
        "        findings = state.get(\"findings\", [])\n",
        "\n",
        "        return f\"\"\"\n",
        "        Executive Summary: {topic.title}\n",
        "\n",
        "        This research investigated {topic.title} in the domain of {topic.domain}.\n",
        "        Key findings include {len(findings)} major insights covering methodology,\n",
        "        results and recommendations. The research demonstrates {'strong' if len(findings) > 3 else 'preliminary'}\n",
        "        evidence supporting the initial research questions.\n",
        "        \"\"\"\n",
        "\n",
        "    def _create_literature_review(self, state: ResearchState) -> str:\n",
        "        \"\"\"Create comprehensive literature review\"\"\"\n",
        "        sources = state.get(\"sources\", [])\n",
        "\n",
        "        review = f\"Literature Review\\n\\n\"\n",
        "        review += f\"Based on {len(sources)} sources, the literature reveals:\\n\\n\"\n",
        "\n",
        "        for i, source in enumerate(sources[:10], 1):\n",
        "            review += f\"{i}. {source.get('title', 'Untitled')} \"\n",
        "            review += f\"({source.get('source', 'Unknown source')})\\n\"\n",
        "            review += f\"   Key contribution: {source.get('content', '')[:200]}...\\n\\n\"\n",
        "\n",
        "        return review\n",
        "\n",
        "    def _extract_methodology(self, state: ResearchState) -> str:\n",
        "        \"\"\"Extract methodology from analysis\"\"\"\n",
        "        analysis = state.get(\"analysis\", {})\n",
        "        methods = analysis.get(\"metadata\", {}).get(\"methodologies_identified\", [])\n",
        "\n",
        "        if methods:\n",
        "            return \"Methodologies identified: \" + \", \".join(methods)\n",
        "        return \"Standard literature review and analysis methodology\"\n",
        "\n",
        "    def _identify_limitations(self, state: ResearchState) -> List[str]:\n",
        "        \"\"\"Identify research limitations\"\"\"\n",
        "        limitations = []\n",
        "\n",
        "        if len(state.get(\"sources\", [])) < 5:\n",
        "            limitations.append(\"Limited number of sources\")\n",
        "\n",
        "        if state.get(\"validation_results\", {}).get(\"issues\"):\n",
        "            limitations.extend(state[\"validation_results\"][\"issues\"])\n",
        "\n",
        "        return limitations if limitations else [\"Standard limitations of literature review methodology\"]\n",
        "\n",
        "    def _generate_recommendations(self, state: ResearchState) -> List[str]:\n",
        "        \"\"\"Generate research recommendations\"\"\"\n",
        "        topic = state[\"topic\"]\n",
        "\n",
        "        return [\n",
        "            f\"Further empirical research on {topic.title}\",\n",
        "            \"Longitudinal studies to validate findings\",\n",
        "            \"Cross-disciplinary approaches for comprehensive understanding\",\n",
        "            \"Application of findings in practical contexts\"\n",
        "        ]\n",
        "\n",
        "    def _compile_final_paper(self, state: ResearchState) -> Dict[str, str]:\n",
        "        \"\"\"Compile final research paper sections\"\"\"\n",
        "        return {\n",
        "            \"abstract\": self._generate_executive_summary(state),\n",
        "            \"introduction\": f\"Introduction to {state['topic'].title} research...\",\n",
        "            \"literature_review\": state.get(\"literature_review\", \"\"),\n",
        "            \"methodology\": state.get(\"methodology\", \"\"),\n",
        "            \"findings\": \"\\n\".join([f[\"content\"] for f in state.get(\"findings\", [])]),\n",
        "            \"discussion\": \"Analysis and interpretation of findings...\",\n",
        "            \"limitations\": \"\\n\".join(state.get(\"limitations\", [])),\n",
        "            \"conclusion\": \"Summary of research and implications...\",\n",
        "            \"references\": self._format_citations(state.get(\"citations\", []))\n",
        "        }\n",
        "\n",
        "    def _format_citations(self, citations: List[Dict]) -> str:\n",
        "        \"\"\"Format citations in APA style\"\"\"\n",
        "        formatted = []\n",
        "        for i, citation in enumerate(citations, 1):\n",
        "            formatted.append(\n",
        "                f\"{i}. {citation.get('authors', ['Author'])[0]} et al. \"\n",
        "                f\"({citation.get('published', 'n.d.').split('-')[0]}). \"\n",
        "                f\"{citation.get('title', 'Untitled')}. \"\n",
        "                f\"{citation.get('source', 'Unknown')}.\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    async def run_research(self, topic: ResearchTopic, config: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Execute the complete research workflow\"\"\"\n",
        "        # Initialize state\n",
        "        initial_state = {\n",
        "            \"topic\": topic,\n",
        "            \"messages\": [\n",
        "                SystemMessage(content=\"You are an advanced research assistant.\"),\n",
        "                HumanMessage(content=f\"Research topic: {topic.title}\")\n",
        "            ],\n",
        "            \"research_phase\": \"initiated\",\n",
        "            \"search_results\": {},\n",
        "            \"sources\": [],\n",
        "            \"analysis\": {},\n",
        "            \"literature_review\": \"\",\n",
        "            \"methodology\": \"\",\n",
        "            \"findings\": [],\n",
        "            \"limitations\": [],\n",
        "            \"recommendations\": [],\n",
        "            \"citations\": [],\n",
        "            \"validation_errors\": [],\n",
        "            \"metadata\": {\n",
        "                \"start_time\": datetime.now().isoformat(),\n",
        "                \"topic_id\": str(uuid.uuid4()),\n",
        "                \"config\": config or {}\n",
        "            },\n",
        "            \"agent_logs\": []\n",
        "        }\n",
        "\n",
        "        # Execute the research workflow\n",
        "        final_state = {}\n",
        "\n",
        "        # FIXED: Proper way to stream with checkpointer\n",
        "        config_dict = {\n",
        "            \"configurable\": {\n",
        "                \"thread_id\": str(uuid.uuid4()),  # Use unique thread_id\n",
        "                \"checkpointer\": self.memory\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async for step in self.app.astream(initial_state, config=config_dict):\n",
        "            for key, value in step.items():\n",
        "                if key != \"__end__\":\n",
        "                    print(f\"Step: {key}\")\n",
        "                    final_state.update(value)\n",
        "\n",
        "        return final_state"
      ],
      "metadata": {
        "id": "_voJWfdhmLxE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_research_improved(result):\n",
        "    \"\"\"Display research output with better formatting and data extraction\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\" ADVANCED RESEARCH REPORT\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Extract topic info better\n",
        "    topic_info = {}\n",
        "    if isinstance(result.get('topic'), dict):\n",
        "        topic_info = result['topic']\n",
        "    elif hasattr(result.get('topic'), 'dict'):  # Pydantic model\n",
        "        topic_info = result['topic'].dict()\n",
        "    elif hasattr(result.get('topic'), 'model_dump'):  # Pydantic v2\n",
        "        topic_info = result['topic'].model_dump()\n",
        "\n",
        "    # 1. RESEARCH OVERVIEW\n",
        "    print(f\"\\n RESEARCH OVERVIEW\")\n",
        "    print(f\"   {''*40}\")\n",
        "    if topic_info:\n",
        "        print(f\"    Topic: {topic_info.get('title', 'N/A')}\")\n",
        "        print(f\"    Domain: {topic_info.get('domain', 'N/A')}\")\n",
        "        print(f\"    Complexity: {topic_info.get('complexity', 'N/A')}\")\n",
        "        if topic_info.get('subtopics'):\n",
        "            print(f\"    Subtopics: {', '.join(topic_info['subtopics'])}\")\n",
        "    else:\n",
        "        print(f\"   Topic data not found in result\")\n",
        "\n",
        "    print(f\"    Research Phase: {result.get('research_phase', 'N/A')}\")\n",
        "    print(f\"    Sources Analyzed: {len(result.get('sources', []))}\")\n",
        "    print(f\"    Key Findings: {len(result.get('findings', []))}\")\n",
        "\n",
        "    # 2. RESEARCH PLAN (from coordinator logs)\n",
        "    print(f\"\\n RESEARCH PLAN\")\n",
        "    print(f\"   {''*40}\")\n",
        "\n",
        "    # Try to find the research plan from coordinator metadata\n",
        "    research_plan = None\n",
        "    if 'metadata' in result and 'research_plan' in result['metadata']:\n",
        "        research_plan = result['metadata']['research_plan']\n",
        "    elif 'analysis' in result and 'comprehensive_analysis' in result['analysis']:\n",
        "        research_plan = result['analysis']['comprehensive_analysis']\n",
        "\n",
        "    if research_plan:\n",
        "        # Extract first few lines\n",
        "        lines = research_plan.split('\\n')[:10]\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                print(f\"   {line}\")\n",
        "        if len(research_plan.split('\\n')) > 10:\n",
        "            print(f\"   ... (truncated, full plan: {len(research_plan.split())} words)\")\n",
        "    else:\n",
        "        print(\"   No detailed research plan found\")\n",
        "\n",
        "    # 3. SOURCES with better formatting\n",
        "    sources = result.get('sources', [])\n",
        "    print(f\"\\n SOURCES FOUND ({len(sources)})\")\n",
        "    print(f\"   {''*40}\")\n",
        "\n",
        "    for i, source in enumerate(sources, 1):\n",
        "        # Extract title\n",
        "        title = source.get('title', 'Untitled')\n",
        "        if title == 'Untitled' and 'content' in source:\n",
        "            # Try to extract title from content\n",
        "            content = source.get('content', '')\n",
        "            if content and len(content.split()) > 2:\n",
        "                title = ' '.join(content.split()[:5]) + '...'\n",
        "\n",
        "        source_type = source.get('source', 'unknown').upper()\n",
        "        relevance = source.get('relevance_score', 0)\n",
        "\n",
        "        print(f\"   {i}. [{source_type}] {title}\")\n",
        "\n",
        "        # Show snippet\n",
        "        if 'content' in source and source['content']:\n",
        "            content = source['content']\n",
        "            snippet = content[:150].replace('\\n', ' ')\n",
        "            if len(content) > 150:\n",
        "                snippet += '...'\n",
        "            print(f\"       {snippet}\")\n",
        "\n",
        "        # Show metadata if available\n",
        "        meta_info = []\n",
        "        if 'published' in source:\n",
        "            meta_info.append(f\"Published: {source['published']}\")\n",
        "        if 'authors' in source and source['authors']:\n",
        "            authors = source['authors'][:2]\n",
        "            meta_info.append(f\"Authors: {', '.join(authors)}\" +\n",
        "                           (f\" + {len(source['authors'])-2} more\" if len(source['authors']) > 2 else \"\"))\n",
        "        if relevance:\n",
        "            meta_info.append(f\"Relevance: {relevance:.2f}\")\n",
        "\n",
        "        if meta_info:\n",
        "            print(f\"       {' | '.join(meta_info)}\")\n",
        "\n",
        "    # 4. KEY FINDINGS - improved formatting\n",
        "    findings = result.get('findings', [])\n",
        "    print(f\"\\n KEY FINDINGS ({len(findings)})\")\n",
        "    print(f\"   {''*40}\")\n",
        "\n",
        "    for i, finding in enumerate(findings, 1):\n",
        "        content = finding.get('content', '').strip()\n",
        "        category = finding.get('category', 'observation').upper()\n",
        "        confidence = finding.get('confidence', 0)\n",
        "\n",
        "        # Clean up the content\n",
        "        if content.startswith('#') or content.startswith('##'):\n",
        "            content = content.lstrip('#').strip()\n",
        "\n",
        "        print(f\"   {i}. [{category}]\")\n",
        "        print(f\"      {content}\")\n",
        "\n",
        "        if confidence:\n",
        "            confidence_bar = '' * int(confidence * 10) + '' * (10 - int(confidence * 10))\n",
        "            print(f\"      Confidence: {confidence:.2f} [{confidence_bar}]\")\n",
        "        print()\n",
        "\n",
        "    # 5. ANALYSIS INSIGHTS\n",
        "    if 'analysis' in result:\n",
        "        analysis = result['analysis']\n",
        "        print(f\"\\n ANALYSIS INSIGHTS\")\n",
        "        print(f\"   {''*40}\")\n",
        "\n",
        "        if 'comprehensive_analysis' in analysis:\n",
        "            insights = analysis['comprehensive_analysis']\n",
        "            # Extract key points\n",
        "            lines = insights.split('\\n')\n",
        "            for line in lines[:8]:  # Show first 8 lines\n",
        "                if line.strip() and len(line.strip()) > 20:\n",
        "                    print(f\"    {line.strip()}\")\n",
        "\n",
        "        if 'confidence_score' in analysis:\n",
        "            print(f\"    Overall Confidence Score: {analysis['confidence_score']:.2f}/1.0\")\n",
        "\n",
        "    # 6. LIMITATIONS & RECOMMENDATIONS\n",
        "    limitations = result.get('limitations', [])\n",
        "    if limitations:\n",
        "        print(f\"\\n IDENTIFIED LIMITATIONS\")\n",
        "        print(f\"   {''*40}\")\n",
        "        for i, limitation in enumerate(limitations, 1):\n",
        "            print(f\"   {i}. {limitation}\")\n",
        "\n",
        "    recommendations = result.get('recommendations', [])\n",
        "    if recommendations:\n",
        "        print(f\"\\n RESEARCH RECOMMENDATIONS\")\n",
        "        print(f\"   {''*40}\")\n",
        "        for i, recommendation in enumerate(recommendations, 1):\n",
        "            print(f\"   {i}. {recommendation}\")\n",
        "\n",
        "    # 7. FINAL PAPER SECTIONS\n",
        "    if 'final_paper' in result:\n",
        "        paper = result['final_paper']\n",
        "        print(f\"\\n FINAL RESEARCH PAPER\")\n",
        "        print(f\"   {''*40}\")\n",
        "        print(f\"   Title: {paper.get('title', 'Research Report')}\")\n",
        "        print(f\"   Status:  COMPLETED\")\n",
        "        print(f\"   Word Count: {paper.get('word_count', 0):,}\")\n",
        "        print(f\"   Citations: {paper.get('citations_count', 0)}\")\n",
        "\n",
        "        # Show paper structure\n",
        "        if 'sections' in paper:\n",
        "            print(f\"   Sections: {len(paper['sections'])}\")\n",
        "            print(f\"   {''*40}\")\n",
        "            for section in paper['sections'][:5]:  # First 5 sections\n",
        "                print(f\"    {section}\")\n",
        "            if len(paper['sections']) > 5:\n",
        "                print(f\"    ... and {len(paper['sections']) - 5} more\")\n",
        "\n",
        "        # Preview abstract if available\n",
        "        if 'content' in paper and 'abstract' in paper['content']:\n",
        "            abstract = paper['content']['abstract']\n",
        "            print(f\"\\n   ABSTRACT PREVIEW:\")\n",
        "            print(f\"   {''*40}\")\n",
        "            lines = abstract.split('\\n')[:4]\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    print(f\"   {line}\")\n",
        "            if len(abstract) > 500:\n",
        "                print(f\"   ... (full abstract: {len(abstract)} characters)\")\n",
        "\n",
        "    # 8. EXECUTIVE SUMMARY\n",
        "    print(f\"\\n EXECUTIVE SUMMARY\")\n",
        "    print(f\"   {''*40}\")\n",
        "\n",
        "    # Create a quick summary\n",
        "    summary_parts = []\n",
        "    if topic_info:\n",
        "        summary_parts.append(f\"Research on '{topic_info.get('title', 'the topic')}'\")\n",
        "    summary_parts.append(f\"analyzed {len(sources)} sources\")\n",
        "    summary_parts.append(f\"identified {len(findings)} key findings\")\n",
        "\n",
        "    if limitations:\n",
        "        summary_parts.append(f\"noted {len(limitations)} limitations\")\n",
        "\n",
        "    if recommendations:\n",
        "        summary_parts.append(f\"proposed {len(recommendations)} recommendations\")\n",
        "\n",
        "    print(f\"   This research {' and '.join(summary_parts)}.\")\n",
        "\n",
        "    if 'analysis' in result and 'confidence_score' in result['analysis']:\n",
        "        conf = result['analysis']['confidence_score']\n",
        "        confidence_level = \"High\" if conf > 0.7 else \"Moderate\" if conf > 0.5 else \"Preliminary\"\n",
        "        print(f\"   Overall confidence: {confidence_level} ({conf:.2f}/1.0)\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*100)\n",
        "    print(\" RESEARCH COMPLETE\")\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "id": "nUiPNsBkaNw3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run research and display immediately\n",
        "async def run_and_display():\n",
        "    topic = ResearchTopic(\n",
        "        title=\"Discounted Cash Flow in modern world\",\n",
        "        domain=\"Finance\",\n",
        "        complexity=\"expert\"\n",
        "    )\n",
        "\n",
        "    assistant = ResearchAssistantGraph()\n",
        "    result = await assistant.run_research(topic)\n",
        "\n",
        "    # Display the research\n",
        "    display_research_improved(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Execute\n",
        "import asyncio\n",
        "result = asyncio.run(run_and_display())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsA8RWZnaRtX",
        "outputId": "630dc20b-410e-4458-eb9d-25930da48c1d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2640601077.py:19: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  self.log_activity(\"coordination_started\", {\"topic\": state[\"topic\"].dict()})\n",
            "INFO:research_coordinator:{\"timestamp\": \"2026-01-27T21:49:33.224708\", \"agent\": \"research_coordinator\", \"activity\": \"coordination_started\", \"metadata\": {\"topic\": {\"title\": \"Discounted Cash Flow in modern world\", \"domain\": \"Finance\", \"complexity\": \"expert\", \"subtopics\": []}}}\n",
            "INFO:research_coordinator:{\"timestamp\": \"2026-01-27T21:49:37.884652\", \"agent\": \"research_coordinator\", \"activity\": \"coordination_completed\", \"metadata\": {\"research_plan\": \"Certainly! Below is a comprehensive research plan for the topic **\\\"Discounted Cash Flow in Modern World\\\"** within the domain of Finance, designed at an expert level.\\n\\n---\\n\\n## \\ud83d\\udd0d Research Plan: Discounted Cash Flow in the Modern World\\n\\n---\\n\\n### 1. **Key Research Questions**\\n\\nTo ensure a focused and rigorous investigation, the following key research questions are proposed:\\n\\n1. **How has the methodology of Discounted Cash Flow (DCF) evolved in response to market volatility and macroeconomic shifts?**\\n2. **What are the limitations of DCF in valuing companies and projects in a rapidly changing financial environment?**\\n3. **How do alternative valuation metrics (e.g., real options, ESG factors) interact with DCF in modern decision-making?**\\n4. **What role do technological advancements (e.g., AI, big data) play in improving the accuracy of DCF models today?**\\n5. **How effective are DCF models in assessing the value of sustainable and ESG-driven investments in the modern economy?**\\n6. **What are the challenges faced by analysts when applying DCF in cross-border, emerging market contexts?**\\n\\n---\\n\\n### 2. **Required Data Sources**\\n\\nTo support a comprehensive analysis, the following data sources should be collected:\\n\\n- **Financial Statements**: Annual and quarterly reports of publicly traded companies.\\n- **Market Data**: Publicly available equity prices, interest rates, and foreign exchange rates.\\n- **Economic Indicators**: GDP growth, inflation rates, unemployment rates, and interest rate trends.\\n- **Sector-Specific Data**: Industry-specific growth rates, capital expenditure trends, and regulatory changes.\\n- **Technological and ESG Trends**: Data from industry reports, sustainability indices, and AI-driven analytics platforms.\\n- **Academic and Industry Reports**: Research papers, white papers, and whitepapers on DCF methodologies.\\n- **Archived Case Studies**: Historical projects and corporate valuations with DCF applications.\\n\\n---\\n\\n### 3. **Methodology Outline**\\n\\nA mixed-methods approach is recommended to ensure depth and versatility:\\n\\n#### A. **Quantitative Analysis**\\n- Apply DCF models to historical financial data of a diverse set of companies.\\n- Analyze sensitivity and impact of assumptions (e.g., discount rate, growth rates).\\n- Use scenario analysis and Monte Carlo simulations.\\n\\n#### B. **Qualitative Analysis**\\n- Review academic literature and case studies on DCF adjustments in different economic environments.\\n- Synthesize insights from regulatory changes and technological disruptions.\\n\\n#### C. **Cross-Disciplinary Integration**\\n- Incorporate insights from behavioral finance, macroeconomics, and ESG investing.\\n- Evaluate DCF\\u2019s integration with ESG factors and long-term sustainability goals.\\n\\n#### D. **Case Study Approach**\\n- Analyze DCF applications in industries such as technology, renewable energy, and healthcare.\\n- Compare outcomes before and after major market shifts (e.g., pandemic, inflation spikes).\\n\\n---\\n\\n### 4. **Success Criteria**\\n\\nThe research should meet the following success criteria:\\n\\n- **Methodological Rigor**: Clear delineation of DCF assumptions, data sources, and validation techniques.\\n- **Comprehensiveness**: Coverage of historical and contemporary DCF applications across financial markets.\\n- **Relevance**: Insightful analysis of how DCF adapts to modern challenges such as ESG and digital transformation.\\n- **Originality**: Contribution of new perspectives or frameworks to the existing DCF literature.\\n- **Practical Utility**: Findings that can assist analysts, policymakers, and investors in making informed decisions.\\n\\n---\\n\\n### 5. **Timeline Estimate**\\n\\n| Phase | Duration | Tasks |\\n|-------|----------|-------|\\n| \\ud83d\\udcc5 Week 1\\u20132 | Planning & literature review | Define scope, tools, and methods |\\n| \\ud83d\\udcc5 Week 3\\u20134 | Data collection & gathering | Access and clean financial and macroeconomic data |\\n| \\ud83d\\udcc5 Week 5\\u20136 | Model development | Build DCF models for case studies |\\n| \\ud83d\\udcc5 Week 7\\u20138 | Analysis & synthesis | Quantitative and qualitative integration |\\n| \\ud83d\\udcc5 Week 9\\u201310 | Quality control & peer review | Validate findings, refine approach |\\n| \\ud83d\\udcc5 Week 11 | Final report | Structure, visualization, and recommendations |\\n| \\ud83d\\udcc5 Week 12 | Dissemination | Present findings to stakeholders |\\n\\n**Total Estimated Time:** ~10 weeks (approx. 2\\u20133 months)\\n\\n---\\n\\n### 6. **Coordination Among Specialist Agents**\\n\\nTo ensure alignment and efficiency:\\n\\n- **Finance Analysts**: Lead data collection and DCF modeling.\\n- **Economists**: Provide macroeconomic context and sensitivity analysis.\\n- **Data Scientists**: Assist in interpreting large datasets and applying machine learning techniques.\\n- **ESG Experts**: Evaluate the role of non-financial factors in DCF.\\n- **Project Managers**: Track progress, ensure milestones are met, and manage cross-disciplinary communication.\\n\\nRegular inter-agency meetings and shared documentation platforms (e.g., Notion, Confluence) will support collaboration and transparency.\\n\\n---\\n\\n### 7. **Monitoring Research Progress and Quality**\\n\\n- **Milestones Review**: Weekly progress check-ins with each specialist.\\n- **Peer Review**: Internal and external peer review of models and data sources.\\n- **Feedback Loop**: Incorporate stakeholder feedback to refine methodologies and assumptions.\\n- **Quality Assurance**: Use version control, documentation standards, and audit trails.\\n\\n---\\n\\n### 8. **Synthesis of Final Findings**\\n\\nThe final findings should offer a holistic understanding of how DCF remains a vital tool in modern valuation, while also identifying its limitations and areas for enhancement. The synthesis will be presented in the form of:\\n\\n- A comprehensive framework for DCF application in a volatile, digitized, and ESG-informed world.\\n- Best practices for adapting DCF models to contemporary challenges.\\n- Recommendations for integrative approaches combining DCF with complementary valuation techniques.\\n\\n---\\n\\nLet me know if you'd like a detailed Gantt chart, sample methodology documents, or a literature review draft to accompany this plan.\", \"next_phase\": \"literature_review\", \"assigned_agents\": [\"search_specialist\", \"analyst\"], \"timestamp\": \"2026-01-27T21:49:37.884638\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: research_coordinator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:search_specialist:{\"timestamp\": \"2026-01-27T21:49:39.103675\", \"agent\": \"search_specialist\", \"activity\": \"search_completed\", \"metadata\": {\"queries\": [\"Discounted Cash Flow in modern world recent developments\", \"Finance Discounted Cash Flow in modern world research papers\", \"Discounted Cash Flow in modern world methodology best practices\"], \"results_found\": 3, \"citations_generated\": 3}}\n",
            "/tmp/ipython-input-2640601077.py:163: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  \"topic\": topic.dict(),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: search_specialist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analyst:{\"timestamp\": \"2026-01-27T21:49:42.431962\", \"agent\": \"analyst\", \"activity\": \"analysis_completed\", \"metadata\": {\"topic\": \"Discounted Cash Flow in modern world\", \"sources_analyzed\": 3, \"confidence\": 0.9}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: analyst\n",
            "Step: validator\n",
            "Step: synthesizer\n",
            "Step: writer\n",
            "\n",
            "====================================================================================================\n",
            " ADVANCED RESEARCH REPORT\n",
            "====================================================================================================\n",
            "\n",
            " RESEARCH OVERVIEW\n",
            "   \n",
            "   Topic data not found in result\n",
            "    Research Phase: completed\n",
            "    Sources Analyzed: 3\n",
            "    Key Findings: 5\n",
            "\n",
            " RESEARCH PLAN\n",
            "   \n",
            "   ### Analysis of Discounted Cash Flow in the Modern World\n",
            "   #### 1. **Key Trends and Patterns**\n",
            "   - **Increased Emphasis on Quantitative Valuation**  \n",
            "   The collected data indicates a persistent demand for Disrupted Cash Flow (DCF) methods as core tools in determining a companys or investments intrinsic value. Both historical and contemporary sources emphasize the significance of projecting future cash flows and appropriately discounting them to present valuea central pillar of DCF analysis.\n",
            "   - **Integration of Quantitative Rigor and Qualitative Influence**  \n",
            "   The responses consistently underscore the necessity of balancing quantitative rigor with qualitative judgment. Viewers note that the structure alone informs value, but assumptions about growth rates, discounting, and cash flow projections are crucial. This highlights a shift toward balanced approaches that treat both methods pragmatically rather than dogmatically.\n",
            "   ... (truncated, full plan: 753 words)\n",
            "\n",
            " SOURCES FOUND (3)\n",
            "   \n",
            "   1. [WEB] December 25, 2025 - Following...\n",
            "       December 25, 2025 - Following the stock market crash of 1929, discounted cash flow analysis gained popularity as a valuation method for stocks . Irvin...\n",
            "       Relevance: 0.80\n",
            "   2. [WEB] Mar 4, 2025  Discover...\n",
            "       Mar 4, 2025  Discover how discounted cash flow (DCF) estimates a company's value by discounting future cash flows, enabling smarter investment decisi...\n",
            "       Relevance: 0.80\n",
            "   3. [WEB] Dec 5, 2024  While...\n",
            "       Dec 5, 2024  While the standard DCF approach adjusts for these risks via the discount rate applied to expected cash flows, the  Modern  DCF approac...\n",
            "       Relevance: 0.80\n",
            "\n",
            " KEY FINDINGS (5)\n",
            "   \n",
            "   1. [METHODOLOGY]\n",
            "      While some emphasize meticulous forecasting and precise DCF models, others caution against over-engineering the process. Critiques note potential for model dependence, especially where assumptions (particularly discount rates) skew results. The consensus leans toward avoiding over-reliance on a single method.\n",
            "      Confidence: 0.80 []\n",
            "\n",
            "   2. [METHODOLOGY]\n",
            "      A recurring tension arises between mechanical application of ACF principles and the need for judgmentsuch as selecting appropriate terminal value methods or adjusting for qualitative factors. This underscores mixed academic perspectives on the extent of human discretion in results.\n",
            "      Confidence: 0.80 []\n",
            "\n",
            "   3. [OBSERVATION]\n",
            "      The current data appears reasonably up to late 2025, but theres minimal evidence for longitudinal or cross-global comparisons, which could strengthen conclusions.\n",
            "      Confidence: 0.80 []\n",
            "\n",
            "   4. [RESULT]\n",
            "      The collective tone in sources hints at a dominant Western-centric view. The underrepresentation of non-English or region-specific analyses may limit the global applicability of current findings.\n",
            "      Confidence: 0.80 []\n",
            "\n",
            "   5. [OBSERVATION]\n",
            "      6. **Preliminary Conclusions**\n",
            "      Confidence: 0.80 []\n",
            "\n",
            "\n",
            " ANALYSIS INSIGHTS\n",
            "   \n",
            "    ### Analysis of Discounted Cash Flow in the Modern World\n",
            "    #### 1. **Key Trends and Patterns**\n",
            "    - **Increased Emphasis on Quantitative Valuation**\n",
            "    The collected data indicates a persistent demand for Disrupted Cash Flow (DCF) methods as core tools in determining a companys or investments intrinsic value. Both historical and contemporary sources emphasize the significance of projecting future cash flows and appropriately discounting them to present valuea central pillar of DCF analysis.\n",
            "    - **Integration of Quantitative Rigor and Qualitative Influence**\n",
            "    Overall Confidence Score: 0.90/1.0\n",
            "\n",
            " IDENTIFIED LIMITATIONS\n",
            "   \n",
            "   1. Limited number of sources\n",
            "\n",
            " RESEARCH RECOMMENDATIONS\n",
            "   \n",
            "   1. Further empirical research on Discounted Cash Flow in modern world\n",
            "   2. Longitudinal studies to validate findings\n",
            "   3. Cross-disciplinary approaches for comprehensive understanding\n",
            "   4. Application of findings in practical contexts\n",
            "\n",
            " EXECUTIVE SUMMARY\n",
            "   \n",
            "   This research analyzed 3 sources and identified 5 key findings and noted 1 limitations and proposed 4 recommendations.\n",
            "   Overall confidence: High (0.90/1.0)\n",
            "\n",
            "====================================================================================================\n",
            " RESEARCH COMPLETE\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ]
}